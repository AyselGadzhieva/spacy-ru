{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf8\n",
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import plac\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.gold import GoldCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['embed_size'] = '8000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG = {'device': 1, 'cpu_count': 4}\n",
    "TESTS = False\n",
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "def load_entries(fn): # '../data/datasets/nerus.jsonl.gz'\n",
    "    entries = []\n",
    "    with gzip.open(fn, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            entry = json.loads(line)\n",
    "            entries.append(entry)\n",
    "    return entries\n",
    "    #del entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=GoldCorpus('../../data/syntagrus/ru_syntagrus-ud-train.json', \n",
    "             '../../data/syntagrus/ru_syntagrus-ud-test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.limit = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ__Animacy=Inan|Case=Acc|Degree=Pos|Number=Plur',\n",
       " 'NOUN__Animacy=Inan|Case=Acc|Gender=Masc|Number=Plur',\n",
       " 'NOUN__Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing',\n",
       " 'ADJ__Case=Gen|Degree=Pos|Gender=Fem|Number=Sing',\n",
       " 'NOUN__Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing',\n",
       " 'VERB__Aspect=Imp|Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act',\n",
       " 'ADP___',\n",
       " 'NOUN__Animacy=Inan|Case=Acc|Gender=Masc|Number=Plur',\n",
       " 'PUNCT___']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ = spacy.blank('ru')\n",
    "d, gp = next(g.train_docs(nlp_))\n",
    "gp.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Так все равно у нас с 2000 года не велось реальной подготовки к вступлению в ВТО , и мы к этому не готовы так же , как и восемь лет назад . , <spacy.gold.GoldParse object at 0x7f5714639048>)\n"
     ]
    }
   ],
   "source": [
    "for i in g.train_docs(nlp_):\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.corpus' from '/Projects/nlp/spacy/spacy-ru/utils/corpus.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import utils.corpus\n",
    "reload(utils.corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.corpus import Corpus, tag_morphology, iter_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SynTagRus = Corpus.from_gold('ru', g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list(iter_corpora([SynTagRus]))) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48814 6491\n",
      "733 608\n",
      "294 39\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(SynTagRus.ds_train), len(SynTagRus.ds_test))\n",
    "print(len(SynTagRus.ds_train.pos), len(SynTagRus.ds_test.pos))\n",
    "print(len(SynTagRus.ds_train.dep), len(SynTagRus.ds_test.dep))\n",
    "print(len(SynTagRus.ds_train.ner), len(SynTagRus.ds_test.ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USE_RU2 = False\n",
    "if USE_RU2:\n",
    "    import ru2e\n",
    "    nlp = ru2e.load_ru2('../../ru2e/')\n",
    "else:\n",
    "    nlp = spacy.blank('ru')\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab.morphology.tag_map.clear()\n",
    "nlp.vocab.morphology.tag_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_submodel(nlp, name, rebuild=True):\n",
    "    if name in nlp.pipe_names and rebuild:\n",
    "        nlp.disable_pipes(name)\n",
    "    if name not in nlp.pipe_names:\n",
    "        print(\"Creating new submodel for {}...\".format(name))\n",
    "        submodel = nlp.create_pipe(name)\n",
    "        nlp.add_pipe(submodel)\n",
    "    submodel = nlp.get_pipe(name)\n",
    "    return submodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new submodel for tagger...\n",
      "745\n",
      "Creating new submodel for parser...\n",
      "294\n",
      "Morphology tag map: 745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('tagger', <spacy.pipeline.pipes.Tagger at 0x7f62f18a35c0>),\n",
       " ('parser', <spacy.pipeline.pipes.DependencyParser at 0x7f633453b2e8>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_ner(nlp, *corpora, rebuild=True):\n",
    "    ner = add_submodel(nlp, 'ner', rebuild=rebuild)\n",
    "    for c in iter_corpora(corpora):\n",
    "        for l in c.ents:\n",
    "            ner.add_label(l)\n",
    "\n",
    "def setup_tagger(nlp, *corpora, rebuild=True):\n",
    "    pos = add_submodel(nlp, 'tagger', rebuild=rebuild)\n",
    "    for c in iter_corpora(corpora):\n",
    "        for l in c.pos:\n",
    "            pos.add_label(l, tag_morphology(l))\n",
    "    print(len(pos.labels))\n",
    "\n",
    "def setup_parser(nlp, *corpora, rebuild=True):\n",
    "    dep = add_submodel(nlp, 'parser', rebuild=rebuild)\n",
    "    for c in iter_corpora(corpora):\n",
    "        for l in c.dep:\n",
    "            dep.add_label(l)\n",
    "    print(len(dep.labels))\n",
    "\n",
    "setup_tagger(nlp, SynTagRus, rebuild=True)\n",
    "setup_parser(nlp, SynTagRus, rebuild=True)\n",
    "#setup_ner(nlp, SynTagRus, rebuild=True)\n",
    "print(f\"Morphology tag map: {len(nlp.vocab.morphology.tag_map)}\")\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pluck import pluck, pluck_list, pluck_dict\n",
    "from utils.tqdm import tqdm_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "import pandas\n",
    "pandas.set_option('display.precision', 3) \n",
    "\n",
    "def init_model(model, CFG):\n",
    "    for n, m in model.pipeline:\n",
    "        if m.model is True:\n",
    "            print(f\"Initializing model because of {n}!\")\n",
    "            model.begin_training(**CFG)\n",
    "            break\n",
    "\n",
    "def _evaluate(model, batches):\n",
    "    scorer = Scorer()\n",
    "    for batch in batches:\n",
    "        # print(\"Example batch:\", batch)\n",
    "        orig_docs, golds = zip(*batch)\n",
    "        docs = model.pipe([b.text for b in orig_docs])\n",
    "        for doc, parse in zip(docs, golds):\n",
    "            scorer.score(doc, parse)\n",
    "    return scorer.scores\n",
    "\n",
    "def evaluate(model, dataset, limit=None, batch_size=32):\n",
    "    generator = dataset.iter(model, limit=limit)\n",
    "    batches = tqdm_batches(minibatch(generator, batch_size), total=limit or len(dataset), leave=False)\n",
    "    return _evaluate(model, batches)\n",
    "\n",
    "def evaluate_data_source(model, ds, count=None, batch_size=32):\n",
    "    # enable_entities(model, ds.ds_test.ner)\n",
    "    res = evaluate(model, ds.ds_test, limit=count)\n",
    "    return res\n",
    "\n",
    "def get_ent_scores(res):\n",
    "    return {k:v for k,v in res['ents_per_type'].items() if k in ds.ents}\n",
    "\n",
    "def display_scores(list_of_scores):\n",
    "    if isinstance(list_of_scores, dict):\n",
    "        list_of_scores = [list_of_scores]\n",
    "    display(pandas.DataFrame.from_records(list_of_scores))\n",
    "\n",
    "def display_scores_t(list_of_scores):\n",
    "    if isinstance(list_of_scores, dict):\n",
    "        list_of_scores = [list_of_scores]\n",
    "    display(pandas.DataFrame.from_records(list_of_scores).T)\n",
    "\n",
    "if TESTS or 0:\n",
    "    init_model(nlp, CFG)\n",
    "    scores = evaluate_data_source(nlp, SynTagRus, count=None, batch_size=32)\n",
    "    display_scores(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def get_other_pipes(nlp, *x):\n",
    "    return [pipe for pipe in nlp.pipe_names if pipe not in x]\n",
    "\n",
    "def get_optimizer(model, CFG):\n",
    "    init_model(model, CFG)\n",
    "    if not hasattr(model, '_optimizer'):\n",
    "        model._optimizer = model.resume_training(**CFG)\n",
    "    return model._optimizer\n",
    "\n",
    "def _train_epoch(model, labels, batches):\n",
    "    init_model(model, CFG)\n",
    "    with model.disable_pipes(*get_other_pipes(model, 'tagger', 'parser')):\n",
    "        # print(\"Training only:\", model.pipe_names)\n",
    "        optimizer = get_optimizer(model, CFG)\n",
    "        losses = {}\n",
    "        n_docs = 0\n",
    "        for batch in batches:\n",
    "            texts, anns = zip(*batch)\n",
    "            # enable_entities(model, labels)\n",
    "            model.update(texts, anns, drop=0.2, losses=losses, sgd=optimizer)\n",
    "            n_docs += len(batch)\n",
    "        meta = {'loss_'+k: numpy.log(1e-10 + (v / n_docs)) for k,v in losses.items()}\n",
    "        meta['docs'] = n_docs\n",
    "    # enable_all_entities(model)\n",
    "    return meta\n",
    "\n",
    "def train_epoch(model, ds, batch_size, count=None):\n",
    "    batches = minibatch(ds.iter(model, limit=count), size=size_)\n",
    "    return _train_epoch(model, ds.ner, tqdm_batches(batches, total=count or len(ds), leave=False, drp=0.2))\n",
    "\n",
    "if TESTS or 0:\n",
    "    size_ = compounding(1., 32., 1.001)\n",
    "    meta = train_epoch(nlp, SynTagRus.ds_train, batch_size=size_, count=1000)\n",
    "    display_scores(meta)\n",
    "\n",
    "if TESTS or 0:\n",
    "    display_scores(evaluate_data_source(nlp, SynTagRus))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TESTS or 0:\n",
    "    scorer = spacy.scorer.Scorer()\n",
    "    for doc, parse in tqdm(SynTagRus.ds_test.iter(nlp, limit=1000), total=1000):\n",
    "        doc = nlp(doc.text)\n",
    "        #explacy.print_parse_info(nlp, doc.text)\n",
    "        #parse.tags = [t.split('__', 1)[0] for t in parse.tags]\n",
    "        #print([(dt.tag_, pt) for dt,pt in zip(doc, parse.tags)])\n",
    "        scorer.score(doc, parse, verbose=False)\n",
    "    display_scores(scorer.scores)\n",
    "    #display_scores(evaluate_data_source(nlp, SynTagRus, count=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from notebooks.examples import explacy\n",
    "if TESTS or 0:\n",
    "    doc = nlp(u'Жизнь после двух тысяч первого года на Марсе стала сложней.')\n",
    "    explacy.print_parse_info(nlp, doc.text)\n",
    "    explacy.print_parse_info(nlp, d.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPORA = [SynTagRus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6b328f2ee740ef9d3befaf401f851d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0e5309e37cb4cf3ac8fd0ce870611dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=24407.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-fa7e43816541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCORPORA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mds_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-e865cdfc27ae>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, ds, batch_size, count)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTESTS\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-e865cdfc27ae>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(model, labels, batches)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# enable_entities(model, labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mn_docs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'loss_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Projects/nlp/spacy-ru/.venv/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses, component_cfg)\u001b[0m\n\u001b[1;32m    473\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnn_parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.nn_parser.Parser.update\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_parser_model.pyx\u001b[0m in \u001b[0;36mspacy.syntax._parser_model.ParserStepModel.begin_update.backprop_parser_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Projects/nlp/spacy-ru/.venv/lib/python3.6/site-packages/thinc/neural/_classes/affine.py\u001b[0m in \u001b[0;36mfinish_update\u001b[0;34m(grad__BO, sgd)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad__BO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mgrad__BO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mascontiguousarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad__BO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad__BO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_b\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad__BO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mgrad__BI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad__BO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g.limit = None\n",
    "size_ = compounding(16., 32., 1.0005)\n",
    "for e in tqdm(range(22, 30)):\n",
    "    res = {'epoch': e+1}\n",
    "    for c in CORPORA:\n",
    "        meta = train_epoch(nlp, c.ds_train, batch_size=size_, count=len(c.ds_train)//2)\n",
    "        for k,v in sorted(meta.items()):\n",
    "            res[k] = v\n",
    "    for c in CORPORA:\n",
    "        res.update(evaluate_data_source(nlp, c, count=None))\n",
    "    avg = res.copy()\n",
    "    with nlp.use_params(get_optimizer(nlp, CFG).averages):\n",
    "        for c in CORPORA:\n",
    "            avg.update(evaluate_data_source(nlp, c, count=None))\n",
    "    display_scores([res, avg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# save model to output directory\n",
    "def save_model(nlp, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    nlp.to_disk(output_dir)\n",
    "    print(\"Saved model to\", output_dir)\n",
    "save_model(nlp, '../../ru2_pos_dep_wider_bsz16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer('приветы всем'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(nlp('20 декабря 2019 года на улице Советской, город Новосибирск, мы с Сашей пошли гулять'), \n",
    "                      style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
